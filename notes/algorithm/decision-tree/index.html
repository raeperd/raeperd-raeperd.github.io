<!doctype html><html lang=korean dir=auto>
<head><meta charset=utf-8>
<meta http-equiv=x-ua-compatible content="IE=edge">
<meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no">
<meta name=robots content="index, follow">
<title>Decision tree | raeperd.github.io</title>
<meta name=keywords content="machine-learning,algorithm">
<meta name=description content="Motivation 데이터를 다른 그룹으로 분류하는 문제를 풀 때, 가장 기본적으로 생각할 수 있는 방법은 여러개의 if~ else~ 문을 반복하는 것입니다. 날씨가 좋고, 평일이 아니면 놀기 좋은날이다! 와 같이 생각할 수 있습니다. 하지만 충분히 많은 데이터를 올바르게 분류하는 if~ else~ 코드는 직접 작성하기에 어려운 부분이 많이 있습니다.
Decision tree는 이런 if~ else~ 문을 자동으로 만들어 내는 알고리즘이라고 할 수 있습니다. 예를 들어 아래와 같은 데이터가 있을때 어떤 조건이 심장병을 성공적으로 분류 할 수 있는지 알아보겠습니다.">
<meta name=author content="raeperd">
<link rel=canonical href=https://raeperd.github.io/notes/algorithm/decision-tree/>
<link href=/assets/css/stylesheet.min.css rel="preload stylesheet" as=style>
<link rel=icon href=https://raeperd.github.io/favicon.ico>
<link rel=icon type=image/png sizes=16x16 href=https://raeperd.github.io/favicon-16x16.png>
<link rel=icon type=image/png sizes=32x32 href=https://raeperd.github.io/favicon-32x32.png>
<link rel=apple-touch-icon href=https://raeperd.github.io/apple-touch-icon.png>
<link rel=mask-icon href=https://raeperd.github.io/safari-pinned-tab.svg>
<meta name=theme-color content="#2e2e33">
<meta name=msapplication-TileColor content="#2e2e33">
<meta name=generator content="Hugo 0.86.1">
<script type=application/javascript>var doNotTrack=!1;doNotTrack||(window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)},ga.l=+new Date,ga('create','UA-168674186-1','auto'),ga('send','pageview'))</script>
<script async src=https://www.google-analytics.com/analytics.js></script>
<meta property="og:title" content="Decision tree">
<meta property="og:description" content="Motivation 데이터를 다른 그룹으로 분류하는 문제를 풀 때, 가장 기본적으로 생각할 수 있는 방법은 여러개의 if~ else~ 문을 반복하는 것입니다. 날씨가 좋고, 평일이 아니면 놀기 좋은날이다! 와 같이 생각할 수 있습니다. 하지만 충분히 많은 데이터를 올바르게 분류하는 if~ else~ 코드는 직접 작성하기에 어려운 부분이 많이 있습니다.
Decision tree는 이런 if~ else~ 문을 자동으로 만들어 내는 알고리즘이라고 할 수 있습니다. 예를 들어 아래와 같은 데이터가 있을때 어떤 조건이 심장병을 성공적으로 분류 할 수 있는지 알아보겠습니다.">
<meta property="og:type" content="article">
<meta property="og:url" content="https://raeperd.github.io/notes/algorithm/decision-tree/"><meta property="og:image" content="https://raeperd.github.io/papermod-cover.png"><meta property="article:section" content="notes">
<meta property="article:published_time" content="2019-10-25T00:00:00+00:00">
<meta property="article:modified_time" content="2019-10-25T00:00:00+00:00"><meta property="og:site_name" content="raeperd.github.io">
<meta name=twitter:card content="summary_large_image">
<meta name=twitter:image content="https://raeperd.github.io/papermod-cover.png">
<meta name=twitter:title content="Decision tree">
<meta name=twitter:description content="Motivation 데이터를 다른 그룹으로 분류하는 문제를 풀 때, 가장 기본적으로 생각할 수 있는 방법은 여러개의 if~ else~ 문을 반복하는 것입니다. 날씨가 좋고, 평일이 아니면 놀기 좋은날이다! 와 같이 생각할 수 있습니다. 하지만 충분히 많은 데이터를 올바르게 분류하는 if~ else~ 코드는 직접 작성하기에 어려운 부분이 많이 있습니다.
Decision tree는 이런 if~ else~ 문을 자동으로 만들어 내는 알고리즘이라고 할 수 있습니다. 예를 들어 아래와 같은 데이터가 있을때 어떤 조건이 심장병을 성공적으로 분류 할 수 있는지 알아보겠습니다.">
<script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Notes","item":"https://raeperd.github.io/notes/"},{"@type":"ListItem","position":2,"name":"Decision tree","item":"https://raeperd.github.io/notes/algorithm/decision-tree/"}]}</script>
<script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Decision tree","name":"Decision tree","description":"Motivation 데이터를 다른 그룹으로 분류하는 문제를 풀 때, 가장 기본적으로 생각할 수 있는 방법은 여러개의 if~ else~ 문을 반복하는 것입니다. 날씨가 좋고, 평일이 아니면 놀기 좋은날이다! 와 같이 생각할 수 있습니다. 하지만 충분히 많은 데이터를 올바르게 분류하는 if~ else~ 코드는 직접 작성하기에 어려운 부분이 많이 있습니다.\nDecision tree는 이런 if~ else~ 문을 자동으로 만들어 내는 알고리즘이라고 할 수 있습니다. 예를 들어 아래와 같은 데이터가 있을때 어떤 조건이 심장병을 성공적으로 분류 할 수 있는지 알아보겠습니다.","keywords":["machine-learning","algorithm"],"articleBody":"Motivation 데이터를 다른 그룹으로 분류하는 문제를 풀 때, 가장 기본적으로 생각할 수 있는 방법은 여러개의 if~ else~ 문을 반복하는 것입니다. 날씨가 좋고, 평일이 아니면 놀기 좋은날이다! 와 같이 생각할 수 있습니다. 하지만 충분히 많은 데이터를 올바르게 분류하는 if~ else~ 코드는 직접 작성하기에 어려운 부분이 많이 있습니다.\nDecision tree는 이런 if~ else~ 문을 자동으로 만들어 내는 알고리즘이라고 할 수 있습니다. 예를 들어 아래와 같은 데이터가 있을때 어떤 조건이 심장병을 성공적으로 분류 할 수 있는지 알아보겠습니다.\n   Chest Pain Good Blood Circulation Blocked Arteries Heart Disease     NO NO NO NO   YES YES YES YES   YES YES NO NO   YES NO NO YES   etc.. etc.. etc.. etc..    최종적으로 아래와 같은 Tree를 만드는 것이 목표입니다. Tree 의 leaf 들을 YES / NO 의 숫자를 대변합니다.\n문제는 이런 트리를 만드는 과정이 생각보다 어렵다는 것입니다. 왜 Wind 나 Humidity 를 기준으로 먼저 분류를 하는 것이 아니라 Weather를 먼저 사용했는지 등의 문제가 있습니다. 이에 대한 자세한 설명은 영상을 통해 확인할 수 있습니다.\nAlgorithm 알고리즘을 간단히 요약하면 아래와 같습니다.\n 변수(Column) 하나를 가지고 각각 이진 트리를 만든다. Gini index, entropy 와 같은 지표를 활용해 어떤 변수가 가장 데이터를 잘 분류했는지 평가한다. (에제는 Gini index를 사용) 트리의 leaf 에 대해 반복하다가 지표가 최적이 되는 시점에 종료한다.  1. 변수(Column) 하나를 가지고 각각 이진 트리를 만든다.  우선 하나의 변수만을 가지고 이진 분류를 해본다. 각각의 변수에 대해 모두 이진 분류를 한 트리를 만든다.    결측값(missing value)이 있을 수 있기 때문에 각각의 합계는 다를 수 있다.\n  어떠한 노드도 100% 의 결과를 가지는 leaf 가 없기 때문에 각각의 leaf 는 impure 하다.\n  어떤 분류가 가장 좋은지 판단하기 위해 impurity 를 측정할 만한 척도가 필요하다.\n- Gini index !\n  ​\n2. Gini index, entropy 와 같은 지표를 활용해 어떤 변수가 가장 데이터를 잘 분류했는지 평가한다. Gini index  낮을 수록 좋다. 계산하기 쉽다.  먼저 Chest Pain에 대한 Gini index 를 계산해보면 아래와 같다.\n$$ \\begin{align} \\text{For left leaf, the Gini impurity} \u0026= 1 - (\\text{the probability of “yes”})^2 - (\\text{the probability of “no”})^2\\\n\u0026= 1 - (\\frac{105}{105+39})^2 - (\\frac{105}{105+39})^2\t\\\n\u0026= 0.395 \\end{align} $$\n$$ \\begin{align} \\text{For right leaf, the Gini impurity} \u0026= 1 - (\\text{the probability of “yes”})^2 - (\\text{the probability of “no”})^2 \\\n\u0026= 1 - (\\frac{34}{34+125})^2 - (\\frac{125}{34+125})^2\t\\\n\u0026= 0.336 \\end{align} $$\n​\n3. 트리의 leaf 에 대해 반복하다가 지표가 최적이 되는 시점에 종료한다. 다음으로 트리 전체의 Gini index 를 계산한다.\n 각 leaf 의 크기가 다르기 때문에 전체의 gini index 는 gini index의 weighted average가 되어야 한다.  $$ \\begin{align} \\text{Gini impurity for Chest Pain} \u0026= \\text{weighted average of Gini impurities for the leaf node}\\\n\u0026= (\\frac{144}{144+159}) \\times 0.395 + (\\frac{159}{144+159}) \\times 0.336 \\\n\u0026= 0.364 \\end{align} $$\n  각각의 leaf 에 대해 동일한 과정을 반복한다.\n  분리한 뒤의 gini index 와 분리하기 전의 gini index 를 비교해 알고리즘을 끝낼 것인지 판단한다.\n  응용 numeric data   Sort by numeric data\n  사이사이에서 평균을 구한다. (아래의 그림 참고)\n  각각의 평균에서의 impurity value 를 계산한다.\n  최선의 impurity value 에서 분기를 친다.\n  ranked data  value 를 less equal 기준으로 분류한다.  multi class  choice A , choice B, choice C, choice A or B, choice B or C, choice C or A 를 기준으로 분류한다.  프로젝트에서 사용할 수 있는 방안 장점  비교적 직관적이고 단순한 알고리즘이다. 다른 모델과 달리 샘플의 분류 기준을 명확하게 알 수 있다.  단점  실제로 Decision Tree 단독으로는 그렇게 좋은 성능을 내주지 못한다. 데이터 셋 자체를 분류하는 것에는 훌륭하지만 새로운 샘플을 잘 분류해주지는 못한다.  이를 Random Forest 를 이용해 해결할 수 있고, 실제 사례도 있다.    자세한 내용은 아래 참고 사이트를 보시면 추가 설명을 확인할 수 있습니다. 특히 전체적인 이해는 영상을 통해 확인해보시는 것을 추천드립니다.\n참고  Using Gini-index for Feature Weighting in Text Categorization\n","wordCount":"581","inLanguage":"korean","datePublished":"2019-10-25T00:00:00Z","dateModified":"2019-10-25T00:00:00Z","author":{"@type":"Person","name":"raeperd"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://raeperd.github.io/notes/algorithm/decision-tree/"},"publisher":{"@type":"Organization","name":"raeperd.github.io","logo":{"@type":"ImageObject","url":"https://raeperd.github.io/favicon.ico"}}}</script>
</head>
<body id=top>
<script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add('dark'):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove('dark'):window.matchMedia('(prefers-color-scheme: dark)').matches&&document.body.classList.add('dark')</script>
<noscript>
<style type=text/css>#theme-toggle,.top-link{display:none}</style>
<style>@media(prefers-color-scheme:dark){:root{--theme:#1d1e20;--entry:#2e2e33;--primary:rgba(255, 255, 255, 0.84);--secondary:rgba(255, 255, 255, 0.56);--tertiary:rgba(255, 255, 255, 0.16);--content:rgba(255, 255, 255, 0.74);--hljs-bg:#2e2e33;--code-bg:#37383e;--border:#333}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style>
</noscript>
<header class=header>
<nav class=nav>
<div class=logo>
<a href=https://raeperd.github.io/ accesskey=h title="raeperd.github.io (Alt + H)">raeperd.github.io</a>
<span class=logo-switches>
<button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg>
</button>
</span>
</div>
<ul id=menu onscroll=menu_on_scroll()>
<li>
<a href=https://raeperd.github.io/articles title=Article>
<span>Article</span>
</a>
</li>
<li>
<a href=https://raeperd.github.io/notes title=Note>
<span>Note</span>
</a>
</li>
<li>
<a href=https://raeperd.github.io/search title="Search (Alt + /)" accesskey=/>
<span>Search</span>
</a>
</li>
<li>
<a href=https://raeperd.github.io/archives/ title=Archives>
<span>Archives</span>
</a>
</li></ul>
</nav>
</header>
<main class=main><article class=post-single>
<header class=post-header>
<h1 class=post-title>
Decision tree
</h1>
<div class=post-meta>
October 25, 2019&nbsp;·&nbsp;raeperd
</div>
<br>
<ul class=post-tags>
<li><a href=https://raeperd.github.io/tags/machine-learning/>machine-learning</a></li>
<li><a href=https://raeperd.github.io/tags/algorithm/>algorithm</a></li>
</ul>
</header>
<div class=post-content>
<h1 id=motivation>Motivation<a hidden class=anchor aria-hidden=true href=#motivation>#</a></h1>
<p>데이터를 다른 그룹으로 분류하는 문제를 풀 때, 가장 기본적으로 생각할 수 있는 방법은 여러개의 if~ else~ 문을 반복하는 것입니다. 날씨가 좋고, 평일이 아니면 놀기 좋은날이다! 와 같이 생각할 수 있습니다. 하지만 충분히 많은 데이터를 올바르게 분류하는 if~ else~ 코드는 직접 작성하기에 어려운 부분이 많이 있습니다.</p>
<p>Decision tree는 이런 if~ else~ 문을 자동으로 만들어 내는 알고리즘이라고 할 수 있습니다. 예를 들어 아래와 같은 데이터가 있을때 어떤 조건이 심장병을 성공적으로 분류 할 수 있는지 알아보겠습니다.</p>
<table>
<thead>
<tr>
<th>Chest Pain</th>
<th>Good Blood Circulation</th>
<th>Blocked Arteries</th>
<th>Heart Disease</th>
</tr>
</thead>
<tbody>
<tr>
<td>NO</td>
<td>NO</td>
<td>NO</td>
<td>NO</td>
</tr>
<tr>
<td>YES</td>
<td>YES</td>
<td>YES</td>
<td>YES</td>
</tr>
<tr>
<td>YES</td>
<td>YES</td>
<td>NO</td>
<td>NO</td>
</tr>
<tr>
<td>YES</td>
<td>NO</td>
<td>NO</td>
<td>YES</td>
</tr>
<tr>
<td>etc..</td>
<td>etc..</td>
<td>etc..</td>
<td>etc..</td>
</tr>
</tbody>
</table>
<p>최종적으로 아래와 같은 Tree를 만드는 것이 목표입니다. Tree 의 leaf 들을 YES / NO 의 숫자를 대변합니다.</p>
<p><img loading=lazy src=https://i.ibb.co/zHC60Zy/1569549456005.png alt=1569549456005>
</p>
<p>문제는 이런 트리를 만드는 과정이 생각보다 어렵다는 것입니다. 왜 Wind 나 Humidity 를 기준으로 먼저 분류를 하는 것이 아니라 Weather를 먼저 사용했는지 등의 문제가 있습니다. 이에 대한 자세한 설명은
<a href="https://www.youtube.com/user/joshstarmer/search?query=Decision+Tree" target=_blank>영상</a>을 통해 확인할 수 있습니다.</p>
<h2 id=algorithm>Algorithm<a hidden class=anchor aria-hidden=true href=#algorithm>#</a></h2>
<p>알고리즘을 간단히 요약하면 아래와 같습니다.</p>
<ol>
<li>변수(Column) 하나를 가지고 각각 이진 트리를 만든다.</li>
<li>Gini index, entropy 와 같은 지표를 활용해 어떤 변수가 가장 데이터를 잘 분류했는지 평가한다. (에제는 Gini index를 사용)</li>
<li>트리의 leaf 에 대해 반복하다가 지표가 최적이 되는 시점에 종료한다.</li>
</ol>
<h3 id=1-변수column-하나를-가지고-각각-이진-트리를-만든다>1. 변수(Column) 하나를 가지고 각각 이진 트리를 만든다.<a hidden class=anchor aria-hidden=true href=#1-변수column-하나를-가지고-각각-이진-트리를-만든다>#</a></h3>
<p><img loading=lazy src=https://i.ibb.co/gV6PcYL/1569487506732.png alt=1569487506732>
</p>
<ul>
<li>우선 하나의 변수만을 가지고 이진 분류를 해본다.</li>
<li>각각의 변수에 대해 모두 이진 분류를 한 트리를 만든다.</li>
</ul>
<p><img loading=lazy src=https://i.ibb.co/LRjvgfv/1569487637982.png alt=1569487637982>
</p>
<ul>
<li>
<p>결측값(missing value)이 있을 수 있기 때문에 각각의 합계는 다를 수 있다.</p>
</li>
<li>
<p>어떠한 노드도 100% 의 결과를 가지는 leaf 가 없기 때문에 각각의 leaf 는 impure 하다.</p>
</li>
<li>
<p>어떤 분류가 가장 좋은지 판단하기 위해 impurity 를 측정할 만한 척도가 필요하다.</p>
<p>-> <strong>Gini index</strong> !</p>
</li>
</ul>
<p>​</p>
<h3 id=2-gini-index-entropy-와-같은-지표를-활용해-어떤-변수가-가장-데이터를-잘-분류했는지-평가한다>2. Gini index, entropy 와 같은 지표를 활용해 어떤 변수가 가장 데이터를 잘 분류했는지 평가한다.<a hidden class=anchor aria-hidden=true href=#2-gini-index-entropy-와-같은-지표를-활용해-어떤-변수가-가장-데이터를-잘-분류했는지-평가한다>#</a></h3>
<h4 id=gini-index>Gini index<a hidden class=anchor aria-hidden=true href=#gini-index>#</a></h4>
<ul>
<li>낮을 수록 좋다.</li>
<li>계산하기 쉽다.</li>
</ul>
<p>먼저 Chest Pain에 대한 Gini index 를 계산해보면 아래와 같다.</p>
<p><img loading=lazy src=https://i.ibb.co/yq13FPc/1569485863162.png alt=1569485863162>
</p>
<p>$$
\begin{align}
\text{For left leaf, the Gini impurity}
&= 1 - (\text{the probability of &ldquo;yes&rdquo;})^2 - (\text{the probability of &ldquo;no&rdquo;})^2\<br>
&= 1 - (\frac{105}{105+39})^2 - (\frac{105}{105+39})^2 \<br>
&= 0.395
\end{align}
$$</p>
<p>$$
\begin{align}
\text{For right leaf, the Gini impurity}
&= 1 - (\text{the probability of &ldquo;yes&rdquo;})^2 - (\text{the probability of &ldquo;no&rdquo;})^2 \<br>
&= 1 - (\frac{34}{34+125})^2 - (\frac{125}{34+125})^2 \<br>
&= 0.336
\end{align}
$$</p>
<p>​</p>
<h3 id=3-트리의-leaf-에-대해-반복하다가-지표가-최적이-되는-시점에-종료한다>3. 트리의 leaf 에 대해 반복하다가 지표가 최적이 되는 시점에 종료한다.<a hidden class=anchor aria-hidden=true href=#3-트리의-leaf-에-대해-반복하다가-지표가-최적이-되는-시점에-종료한다>#</a></h3>
<p>다음으로 트리 전체의 Gini index 를 계산한다.</p>
<ul>
<li>각 leaf 의 크기가 다르기 때문에 전체의 gini index 는 gini index의 weighted average가 되어야 한다.</li>
</ul>
<p>$$
\begin{align}
\text{Gini impurity for Chest Pain} &= \text{weighted average of Gini impurities for the leaf node}\<br>
&= (\frac{144}{144+159}) \times 0.395 + (\frac{159}{144+159}) \times 0.336 \<br>
&= 0.364
\end{align}
$$</p>
<ul>
<li>
<p>각각의 leaf 에 대해 동일한 과정을 반복한다.</p>
</li>
<li>
<p>분리한 뒤의 gini index 와 분리하기 전의 gini index 를 비교해 알고리즘을 끝낼 것인지 판단한다.</p>
</li>
</ul>
<h3 id=응용>응용<a hidden class=anchor aria-hidden=true href=#응용>#</a></h3>
<h4 id=numeric-data>numeric data<a hidden class=anchor aria-hidden=true href=#numeric-data>#</a></h4>
<ol>
<li>
<p>Sort by numeric data</p>
</li>
<li>
<p>사이사이에서 평균을 구한다. (아래의 그림 참고)</p>
</li>
<li>
<p>각각의 평균에서의 impurity value 를 계산한다.</p>
</li>
<li>
<p>최선의 impurity value 에서 분기를 친다.</p>
</li>
</ol>
<p><img loading=lazy src=https://i.ibb.co/2nptTwT/1569488629706.png alt=1569488629706>
</p>
<h4 id=ranked-data>ranked data<a hidden class=anchor aria-hidden=true href=#ranked-data>#</a></h4>
<ul>
<li>value 를 less equal 기준으로 분류한다.</li>
</ul>
<h4 id=multi-class>multi class<a hidden class=anchor aria-hidden=true href=#multi-class>#</a></h4>
<ul>
<li>choice A , choice B, choice C, choice A or B, choice B or C, choice C or A 를 기준으로 분류한다.</li>
</ul>
<h2 id=프로젝트에서-사용할-수-있는-방안>프로젝트에서 사용할 수 있는 방안<a hidden class=anchor aria-hidden=true href=#프로젝트에서-사용할-수-있는-방안>#</a></h2>
<h3 id=장점>장점<a hidden class=anchor aria-hidden=true href=#장점>#</a></h3>
<ul>
<li>비교적 직관적이고 단순한 알고리즘이다.</li>
<li>다른 모델과 달리 샘플의 분류 기준을 명확하게 알 수 있다.</li>
</ul>
<h3 id=단점>단점<a hidden class=anchor aria-hidden=true href=#단점>#</a></h3>
<ul>
<li>실제로 Decision Tree 단독으로는 그렇게 좋은 성능을 내주지 못한다.</li>
<li>데이터 셋 자체를 분류하는 것에는 훌륭하지만 새로운 샘플을 잘 분류해주지는 못한다.
<ul>
<li>이를 <strong>Random Forest</strong> 를 이용해 해결할 수 있고, 실제 사례도 있다.</li>
</ul>
</li>
</ul>
<p>자세한 내용은 아래 참고 사이트를 보시면 추가 설명을 확인할 수 있습니다. 특히 전체적인 이해는
<a href="https://www.youtube.com/user/joshstarmer/search?query=Decision+Tree" target=_blank>영상</a>을 통해 확인해보시는 것을 추천드립니다.</p>
<h1 id=참고>참고<a hidden class=anchor aria-hidden=true href=#참고>#</a></h1>
<p>
<a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.353.3216&rep=rep1&type=pdf" target=_blank>Using Gini-index for Feature Weighting in Text Categorization</a></p>
</div>
<footer class=post-footer>
<div class=share-buttons>
<a target=_blank rel="noopener noreferrer" aria-label="share Decision tree on twitter" href="https://twitter.com/intent/tweet/?text=Decision%20tree&url=https%3a%2f%2fraeperd.github.io%2fnotes%2falgorithm%2fdecision-tree%2f&hashtags=machine-learning%2calgorithm"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM195.519 424.544c135.939.0 210.268-112.643 210.268-210.268.0-3.218.0-6.437-.153-9.502 14.406-10.421 26.973-23.448 36.935-38.314-13.18 5.824-27.433 9.809-42.452 11.648 15.326-9.196 26.973-23.602 32.49-40.92-14.252 8.429-30.038 14.56-46.896 17.931-13.487-14.406-32.644-23.295-53.946-23.295-40.767.0-73.87 33.104-73.87 73.87.0 5.824.613 11.494 1.992 16.858-61.456-3.065-115.862-32.49-152.337-77.241-6.284 10.881-9.962 23.601-9.962 37.088.0 25.594 13.027 48.276 32.95 61.456-12.107-.307-23.448-3.678-33.41-9.196v.92c0 35.862 25.441 65.594 59.311 72.49-6.13 1.686-12.72 2.606-19.464 2.606-4.751.0-9.348-.46-13.946-1.38 9.349 29.426 36.628 50.728 68.965 51.341-25.287 19.771-57.164 31.571-91.8 31.571-5.977.0-11.801-.306-17.625-1.073 32.337 21.15 71.264 33.41 112.95 33.41z"/></svg>
</a>
<a target=_blank rel="noopener noreferrer" aria-label="share Decision tree on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&url=https%3a%2f%2fraeperd.github.io%2fnotes%2falgorithm%2fdecision-tree%2f&title=Decision%20tree&summary=Decision%20tree&source=https%3a%2f%2fraeperd.github.io%2fnotes%2falgorithm%2fdecision-tree%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg>
</a>
<a target=_blank rel="noopener noreferrer" aria-label="share Decision tree on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fraeperd.github.io%2fnotes%2falgorithm%2fdecision-tree%2f&title=Decision%20tree"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg>
</a>
<a target=_blank rel="noopener noreferrer" aria-label="share Decision tree on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fraeperd.github.io%2fnotes%2falgorithm%2fdecision-tree%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg>
</a>
<a target=_blank rel="noopener noreferrer" aria-label="share Decision tree on whatsapp" href="https://api.whatsapp.com/send?text=Decision%20tree%20-%20https%3a%2f%2fraeperd.github.io%2fnotes%2falgorithm%2fdecision-tree%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg>
</a>
<a target=_blank rel="noopener noreferrer" aria-label="share Decision tree on telegram" href="https://telegram.me/share/url?text=Decision%20tree&url=https%3a%2f%2fraeperd.github.io%2fnotes%2falgorithm%2fdecision-tree%2f"><svg viewBox="2 2 28 28"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg>
</a>
</div>
</footer><div id=disqus_thread></div>
<script>(function(){var a=document,b=a.createElement('script');b.src='https://raeperd-github-io.disqus.com/embed.js',b.setAttribute('data-timestamp',+new Date),(a.head||a.body).appendChild(b)})()</script>
<noscript>Please enable JavaScript to view the <a href=https://disqus.com/?ref_noscript>comments powered by Disqus.</a></noscript>
</article>
</main><footer class=footer>
<span>&copy; 2021 <a href=https://raeperd.github.io/>raeperd.github.io</a></span>
<span>&#183;</span>
<span>Powered by <a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a></span>
<span>&#183;</span>
<span>Theme <a href=https://git.io/hugopapermod rel=noopener target=_blank>PaperMod</a></span>
</footer>
<a href=#top aria-label="go to top" title="Go to Top (Alt + G)">
<button class=top-link id=top-link type=button accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</button>
</a>
<script defer src=/assets/js/highlight.min.js onload=hljs.initHighlightingOnLoad()></script>
<script>window.onload=function(){localStorage.getItem("menu-scroll-position")&&(document.getElementById('menu').scrollLeft=localStorage.getItem("menu-scroll-position"))};function menu_on_scroll(){localStorage.setItem("menu-scroll-position",document.getElementById('menu').scrollLeft)}document.querySelectorAll('a[href^="#"]').forEach(a=>{a.addEventListener("click",function(b){b.preventDefault();var a=this.getAttribute("href").substr(1);window.matchMedia('(prefers-reduced-motion: reduce)').matches?document.querySelector(`[id='${decodeURIComponent(a)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(a)}']`).scrollIntoView({behavior:"smooth"}),a==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${a}`)})})</script>
<script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script>
<script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove('dark'),localStorage.setItem("pref-theme",'light')):(document.body.classList.add('dark'),localStorage.setItem("pref-theme",'dark'))})</script>
</body>
</html>