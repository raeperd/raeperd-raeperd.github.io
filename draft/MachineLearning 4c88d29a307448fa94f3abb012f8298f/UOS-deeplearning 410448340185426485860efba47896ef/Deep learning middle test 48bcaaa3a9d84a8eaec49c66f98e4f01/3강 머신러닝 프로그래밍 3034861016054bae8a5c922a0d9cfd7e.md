# 3강 머신러닝 프로그래밍

[3강 머신러닝 프로그래밍.pdf](3%E1%84%80%E1%85%A1%E1%86%BC%20%E1%84%86%E1%85%A5%E1%84%89%E1%85%B5%E1%86%AB%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC%20%E1%84%91%E1%85%B3%E1%84%85%E1%85%A9%E1%84%80%E1%85%B3%E1%84%85%E1%85%A2%E1%84%86%E1%85%B5%E1%86%BC%203034861016054bae8a5c922a0d9cfd7e/3__.pdf)

# 머신러닝 프로그래밍의 과정

[3%E1%84%80%E1%85%A1%E1%86%BC%20%E1%84%86%E1%85%A5%E1%84%89%E1%85%B5%E1%86%AB%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC%20%E1%84%91%E1%85%B3%E1%84%85%E1%85%A9%E1%84%80%E1%85%B3%E1%84%85%E1%85%A2%E1%84%86%E1%85%B5%E1%86%BC%203034861016054bae8a5c922a0d9cfd7e/untitled](3%E1%84%80%E1%85%A1%E1%86%BC%20%E1%84%86%E1%85%A5%E1%84%89%E1%85%B5%E1%86%AB%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC%20%E1%84%91%E1%85%B3%E1%84%85%E1%85%A9%E1%84%80%E1%85%B3%E1%84%85%E1%85%A2%E1%84%86%E1%85%B5%E1%86%BC%203034861016054bae8a5c922a0d9cfd7e/untitled)

# Scikit-learn

[3%E1%84%80%E1%85%A1%E1%86%BC%20%E1%84%86%E1%85%A5%E1%84%89%E1%85%B5%E1%86%AB%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC%20%E1%84%91%E1%85%B3%E1%84%85%E1%85%A9%E1%84%80%E1%85%B3%E1%84%85%E1%85%A2%E1%84%86%E1%85%B5%E1%86%BC%203034861016054bae8a5c922a0d9cfd7e/untitled%201](3%E1%84%80%E1%85%A1%E1%86%BC%20%E1%84%86%E1%85%A5%E1%84%89%E1%85%B5%E1%86%AB%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC%20%E1%84%91%E1%85%B3%E1%84%85%E1%85%A9%E1%84%80%E1%85%B3%E1%84%85%E1%85%A2%E1%84%86%E1%85%B5%E1%86%BC%203034861016054bae8a5c922a0d9cfd7e/untitled%201)

- Trainning Data : Test Data  0.8~0.7 : 0.2~0.3 정도의 비율

# 분류 알고리즘 종류

- logistic regression
- Decision Tree
- Random Forest
- neural Network
- Suport vector machine
- k-nearist neighbors (KNN)
- Naive Bayesian Classification

# SVM Algorithm (Support Vector Machine)

## 모듈 및 패키지 활용

```python
import math 
import pandas as pd
from sklearn import svm, metrics

import math
print("원주율은 %.10f 입니다" % math.pi)
print("원주율은 {0} 입니다".format(math.pi))
print(math.sqrt(4.0))

# math -> m으로 이름 변환하여 사용
```

## Some Tips

- `tab` 추천
- `shift` + `tab` : 도움말

# Code Example

## Scikit-learn을 이용한 머신러닝 (using svm)

```python
import pandas as pd
from sklearn import svm, metrics
# XOR 연산
xor_input = [
    [0, 0, 0],
    [0, 1, 1],
    [1, 0, 1],
    [1, 1, 0]
]

xor_df = pd.DataFrame(xor_input)
print(xor_df, "\n")

# 입력을 학습 전용 데이터와 테스트 전용 데이터로 분류하기 
xor_data  = xor_df[[0,1]] # 데이터 (COLUMN 선택)
xor_label = xor_df[2]   # 레이블

print(xor_data)
    
# 데이터 학습과 예측하기 (SVM)
clf = svm.SVC()
clf.fit(xor_data, xor_label)
pre = clf.predict(xor_data)

# 정답률 구하기 
ac_score = metrics.accuracy_score(xor_label, pre)
print("정답률 =", ac_score)
```

## SVM을 이용한 iris 데이터 분류

```python
import pandas as pd
from sklearn import svm, metrics
from sklearn.model_selection import train_test_split
# 붓꽃의 CSV 데이터 읽어 들이기 --- (※1)
csv = pd.read_csv('iris.csv')
print(csv)

# 필요한 열 추출하기 --- (※2) 
# X와 Y를 구분
csv_data = csv[["SepalLength","SepalWidth","PetalLength","PetalWidth"]]
print(csv_data)
csv_label = csv["Name"]

# 학습 전용 데이터와 테스트 전용 데이터로 나누기 --- (※3)
train_data, test_data, train_label, test_label = train_test_split(csv_data, csv_label)

# 데이터 학습시키고 예측하기 --- (※4)
clf = svm.SVC()
clf.fit(train_data, train_label)
pre = clf.predict(test_data)

# 정답률 구하기 --- (※5)
ac_score = metrics.accuracy_score(test_label, pre)
print("정답률 =", ac_score)
```

- `svm.SVC()` 에 커널함수를 변경하는 함수를 포함한다.
- `slack` 변수는 완전한 선형분리가 불가능할때 허용하는 오차의 값
    - 많은 경우 이 값이 변하더라도 커널 함수와 달리 큰차이는 없음

# 나이브 베이즈 (Naive Bayes) 분류

## 확률에 기반한 분류 기법

- 복잡한 조건부 확률 계산을 피하기 위해 각 확률변수들이 모두 독립이라고 가정한다.

[3%E1%84%80%E1%85%A1%E1%86%BC%20%E1%84%86%E1%85%A5%E1%84%89%E1%85%B5%E1%86%AB%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC%20%E1%84%91%E1%85%B3%E1%84%85%E1%85%A9%E1%84%80%E1%85%B3%E1%84%85%E1%85%A2%E1%84%86%E1%85%B5%E1%86%BC%203034861016054bae8a5c922a0d9cfd7e/untitled%202](3%E1%84%80%E1%85%A1%E1%86%BC%20%E1%84%86%E1%85%A5%E1%84%89%E1%85%B5%E1%86%AB%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC%20%E1%84%91%E1%85%B3%E1%84%85%E1%85%A9%E1%84%80%E1%85%B3%E1%84%85%E1%85%A2%E1%84%86%E1%85%B5%E1%86%BC%203034861016054bae8a5c922a0d9cfd7e/untitled%202)

- 등식의 우변의 분자항은 labeled-dataset 만으로도 계산할 수 있음
- 변수간의 독립을 가정했기 때문에 Naive
- 정확한 확률 값이 중요한 것이 아니라 확률값의 대소가 중요하기 때문에 무모한 가정에도 불구하고 꽤 좋은 결과를 얻을 수 있음
- 변수가 많은 경우 효과적으로 동작한다.

## NB의 종류

- Gaussian
- Multinomial
- Bernoulli

```python
from sklearn.naive_bayes import GaussianNB, MultinomialNB
################################################
# read data 
# data processing 
# train_test_split
################################################
# NB로 학습시키고 예측하기 --- (※4)
clf = GaussianNB()
clf.fit(train_data, train_label)
pred = clf.predict(test_data)
# 정답률 구하기 --- (※5)
acc_score = metrics.accuracy_score(test_label, pred)
print("정답률 =", acc_score)
print(confusion_matrix(test_label, pred))
```

# k-NN

```python
from sklearn.metrics import confusion_matrix # to use confusion_matrix

################################################
# KNN으로 학습시키고 예측하기 --- (※4)
clf = neighbors.KNeighborsClassifier(n_neighbors=9,  p=3, metric='minkowski')
clf.fit(train_data, train_label)
pred = clf.predict(test_data)
# 정답률 구하기 --- (※5)
acc_score = metrics.accuracy_score(test_label, pred)
print("정답률 =", acc_score)
print(confusion_matrix(test_label, pred))
```

- metric를 minkowski 거리함수로 세팅
    - p=2 이면 Euclidean distance
    - p=3 이면 Manhathan distance
    - p = 3, 4 인 경우 성능이 더 좋아질 수도 있음
- n_neighbors 와 metric 에 따라 알고리즘의 성능이 바뀌게 된다.
- weight는 feature의 비율을 변경할 수 있음
- `confusion_matrix(test_label, pred)` (교차행렬)  (시험나옴)

딥러닝이 최고가 아니다. 알고리즘마다 장점과 특징이 있고 모두 사용할 줄 아는 것이 중요하다. 

우선은 기초 아이디어와 코드를 보자

# Wine 데이터 분류

```python
import pandas as pd
from sklearn import datasets
from sklearn import svm, metrics
from sklearn.naive_bayes import GaussianNB, MultinomialNB
from sklearn import neighbors, metrics, model_selection
#from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import confusion_matrix

#wine = datasets.load_wine()

scaler = MinMaxScaler()           # Scaler we leanred
wine = pd.read_csv('wine.csv')
wine_data = wine.iloc[ : ,1:14]   # 
wine_label = wine["class"]

#print(wine_data.iloc[0:5, :])
#print(type(wine_data))
wine_data = scaler.fit_transform(wine_data) # 모든 데이터를 0과 1사이로 scailing 
#print(type(wine_data))
#print(wine_data[0:5, : ])

train_data, test_data, train_label, test_label = train_test_split(wine_data, wine_label, test_size=0.2)

clf = svm.SVC()
#clf = GaussianNB()
#clf = MultinomialNB()
#clf = neighbors.KNeighborsClassifier()
clf.fit(train_data, train_label)
pred = clf.predict(test_data)
result = pd.DataFrame({"정답": test_label, "예측": pred})
print(result[0:10])
acc_score = metrics.accuracy_score(test_label, pred)
print("Accuracy: %.5f" % acc_score)
print(confusion_matrix(test_label, pred))
#scores = model_selection.cross_val_score(clf, csv_data, csv_label, cv=5)
#print(scores)
#print(scores.mean())
```

- `MinMaxScaler()` 와 `fit_transform` 으로 데이터를 0과 1사이로 스케일링
- class column은 스케일링에서 제외해야 함

# RandomForest Algorithm

- Decision Tree를 여러개 생성하여 선형 결합 (**Ensenble**)
- 해석이 어렵지만 예측력은 매우 높은 방법
- 알고리즘
    1. 주어진 데이터에서 n개의 Bootstrap 표본 생성
    2. 각 표본 별로 입력 변수 k(k<p)개를 무작위로 추출하여 의사결정나무 생성 
    3. 생성된 Decision Tree를 선형 결합 (voting) 
- 모델마다 가중치를 줄 수도 있지만, 어렵겠지 ..? : Boosting
- weak 모델을 여러개 만들어서 vote 체계를 만들 수 있다.
- weak 모델은 위에서 배운 SVM, NB 와 같은 모델
- Ensenble과 같은 여러 모델의 결합을 **Meta Learning** 이라 한다.
- RandomForest 는 Decision Tree의 Ensenble을 효과적으로 발전 시킨 것

## How to make dicision Tree?

- 데이터의 클래스가 섞이지 않을때까지 분류해나가며 만들 수 있다.

```python
import pandas as pd
from sklearn.ensemble import RandomForestClassifier
from sklearn import metrics
from sklearn.model_selection import train_test_split

# 데이터 읽어 들이기
mr = pd.read_csv("mushroom.csv", header=None)
print(mr.head(10))

# label 분리
df = pd.DataFrame(mr.iloc[:, 0]) # 1 column만 선택하면 Series가 되므로 다시 dataframe으로 만듦

# 두번쨰 컬럼부터 마지막 컬럼까지 one-hot encoding하고 label에 붙임
df = df.join(pd.get_dummies(mr.iloc[:, 1:]))
print(df)
```

# 모델 시각화

```python
from mlxtend.plotting import plot_decision_regions
import matplotlib.pyplot as plt
from sklearn import datasets
from sklearn.svm import SVC
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier

# Loading some example data
iris = datasets.load_iris()
X = iris.data[:, [2, 3]]
y = iris.target

# Training a classifier
#clf = SVC(C=0.5, kernel='rbf')
clf = RandomForestClassifier(criterion='entropy', n_estimators=10, random_state=1, n_jobs=2)
clf.fit(X, y)

# Plotting decision regions
plot_decision_regions(X, y, clf=clf, legend=2)

# Adding axes annotations
plt.xlabel('sepal length [cm]')
plt.ylabel('petal length [cm]')
plt.title('Classification Model on Iris')
plt.show()
```

Decision Tree는 다른 알고리즘과 다르게 해석이 가능하다.

해석 가능한 Deeplearning에 대한 연구를 시작하고 있다. Decision Tree와 함께

 

보편적으로 svn이 성능이 좋음

k-NN은 학습을 실직적으로 안함

데이터 자체가 모델 

instance based learning 

lazy learning

# MNIST

## Data Format

[3%E1%84%80%E1%85%A1%E1%86%BC%20%E1%84%86%E1%85%A5%E1%84%89%E1%85%B5%E1%86%AB%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC%20%E1%84%91%E1%85%B3%E1%84%85%E1%85%A9%E1%84%80%E1%85%B3%E1%84%85%E1%85%A2%E1%84%86%E1%85%B5%E1%86%BC%203034861016054bae8a5c922a0d9cfd7e/untitled%203](3%E1%84%80%E1%85%A1%E1%86%BC%20%E1%84%86%E1%85%A5%E1%84%89%E1%85%B5%E1%86%AB%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC%20%E1%84%91%E1%85%B3%E1%84%85%E1%85%A9%E1%84%80%E1%85%B3%E1%84%85%E1%85%A2%E1%84%86%E1%85%B5%E1%86%BC%203034861016054bae8a5c922a0d9cfd7e/untitled%203)

- image set은1바이트씩 18x18 픽셀을 저장
- label set은 image set에서 읽어온 것을 바탕으로 학습한 결과

## 데이터 변환

```python
# 이미지 데이터를 읽고 CSV로 저장하기 
    res = []
    for idx in range(lbl_count):
        if idx > 1000: break   # 1000 개만 load
        label = struct.unpack("B", lbl_f.read(1))[0] # 레이블 데이터 1개 read , B : unsigned char
        bdata = img_f.read(pixels)   # image data 28*28 read
        sdata = list(map(lambda n: str(n), bdata)) # 한 pixel씩 문자열로 변환한 후에 list로 변환
        
        # csv로 저장
        csv_f.write(str(label)+",")
        csv_f.write(",".join(sdata)+"\r\n")
```

## 데이터 전처리

- 정규화
    - 보통 0~1 사이의 실수로 변환
- 문자열로 구성된 특징의 변환
    - 분류 알고리즘에 따라서 문자열 특징을 허용하지 않는 것도 존재
    - 따라서 숫자나 one-hot encoding이 필요함
- Missing Value 처리

### 정규화

```python
import pandas as pd
from sklearn.preprocessing import scale, minmax_scale, MinMaxScaler

x = pd.DataFrame({'col':[14.00,90.20,90.95,96.27,91.21]})
# 평균 0, 분산을 이용해 정규화
# astype(float)는 scale의 입력이 float이므로 warning방지를 위해 변환
x["scale"] = scale(x[['col']])  # z-scoring
    
print(x)
# 0과 과 1 사이의 값으로 변환
x['minmax_scale'] = minmax_scale(x[['col']]) # min-max scaling

print(x)
```

- 보통 0~1 사이의 실수로 변환

> col          scale
0     14.00     -1.995290
1     90.20     0.436356
2     90.95     0.460289
3     96.27     0.630058
4     91.21     0.468586
     col            scale      minmax_scale
0 14.00     -1.995290      0.000000
1 90.20     0.436356        0.926219
2 90.95     0.460289         0.935335
3 96.27     0.630058        1.000000
4 91.21     0.468586        0.938495

### 문자열로 구성된 특징의 변환

```python
from sklearn import preprocessing
le = preprocessing.LabelEncoder()

le.fit(["paris", "paris", "tokyo", "amsterdam", "seoul"])
print(le.classes_)
#print(type(le.classes_), "\n")

data = le.transform(["tokyo", "tokyo", "paris", "amsterdam", "seoul"]) 
print(data)
#print(type(data), "\n")

original = le.inverse_transform([3, 3, 1, 0, 2])
print(original)
#print(type(data))
```

- `scikit-learn` 에 `preprocessing` 라이브러리가 존재함
- 분류 알고르즘에 따라서 문자열 특징을 허용하지 않는 것도 존재
- 따라서 숫자나 one-hot encoding이 필요함

### DataFrame을 한번에 변환하는 방법

```python
from sklearn import preprocessing
import pandas as pd

le = preprocessing.LabelEncoder()
df = pd.DataFrame({'A': ['a', 'b', 'b', 'c', 'a'],
                                       'B': ['x', 'y', 'x', 'y', 'x']})

# fit_transform: fit과 tranform을 동시에 처리
# df.apply는 dataframe에서 인자로 주어진 함수를 각 column에 적용하는 함수
data = df.apply(le.fit_transform)
print(data)
print(type(data), "\n")
```

- 여러 column을 code화 시키는 방법

### One-hot encoding

[3%E1%84%80%E1%85%A1%E1%86%BC%20%E1%84%86%E1%85%A5%E1%84%89%E1%85%B5%E1%86%AB%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC%20%E1%84%91%E1%85%B3%E1%84%85%E1%85%A9%E1%84%80%E1%85%B3%E1%84%85%E1%85%A2%E1%84%86%E1%85%B5%E1%86%BC%203034861016054bae8a5c922a0d9cfd7e/untitled%204](3%E1%84%80%E1%85%A1%E1%86%BC%20%E1%84%86%E1%85%A5%E1%84%89%E1%85%B5%E1%86%AB%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC%20%E1%84%91%E1%85%B3%E1%84%85%E1%85%A9%E1%84%80%E1%85%B3%E1%84%85%E1%85%A2%E1%84%86%E1%85%B5%E1%86%BC%203034861016054bae8a5c922a0d9cfd7e/untitled%204)

- 단순히 숫자로 변환하는 것은 상대적인 값이 영향을 미치므로 되도록 one-hot encoding으로 변환하는 것을 권장함
- 값을 Column으로 바꾼다.
- 거의 100% 이런식으로 one
- 컬럼이 어떻게 변하는지 시험에 나올 수 있다.

### pandas 를 이용한 One-hot encoding

```python
import pandas as pd

df = pd.DataFrame({'country': ['russia', 'germany', 'australia','korea','germany']})
a = pd.get_dummies(df,prefix=['country'])
print(a)

# 여러개의 column을 동시에 변환할 수도 있다.

df = pd.DataFrame({'A': ['a','b','b','c','a'],
									 'B': ['x','y','x','y','x']})
a = pd.get_dummies(df, prefix=['A','B'])
print(a)
```

- 문자의 인코딩은 자동으로 처리해준다.

### Missing value 처리

### open() 함수

- 이미지 파일을 메모리에 적재할 필요가 있다. (= 버퍼가 필요함)
- open 함수가 이것을 해줌