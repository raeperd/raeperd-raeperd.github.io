# 4강 tensorflow 기초

[4강. Tensorflow 기초.pdf](4%E1%84%80%E1%85%A1%E1%86%BC%20tensorflow%20%E1%84%80%E1%85%B5%E1%84%8E%E1%85%A9%205764296313454fc49857119c864e33cb/4._Tensorflow_.pdf)

```python
# 간단한 메시지 출력하기
import tensorflow as tf
hello = tf.constant('Hello, TensorFlow!')
sess = tf.Session()
print(sess.run(hello))
sess.close()
```

```python
import tensorflow as tf

# 상수 정의 (상수 객체 정의)
# a와 b는 변수가 아니라 a와 b를 가리키는 reference
a = tf.constant(1234)
b = tf.constant(5000)
# add 연산 정의 
# add_op는 변수가 아니라 add연산자를 가리키는 reference
add_op = a + b

# 세션 시작하기(계산을 실행하기 위한 세션 실행) 
sess = tf.Session()

# 세션 객체의 run method로 add_op객체의 값 계산하기
res = sess.run(add_op) 
print(res)

# 세션을 열었으면 반드시 close해야함
sess.close()
```

```python
import tensorflow as tf
# 상수 정의하기 
a = tf.constant(2)
b = tf.constant(3)
c = tf.constant(4)
# 연산 정의하기 
calc1_op = a + b * c
calc2_op = (a + b) * c
# 세션 시작하기
# 다음과 같이 실행할 경우 sess.close 생략 가능

with tf.Session()  as sess:
   res1 = sess.run(calc1_op) # 식 평가하기
   print(res1)
   res2 = sess.run(calc2_op) # 식 평가하기
   print(res2)
```

```python
import tensorflow as tf

a = tf.Variable(1)

# 변수객체 a을 10으로 변경하는 assign 연산을 정의
# b는 assign 연산을 기르키는 reference
b = tf.assign(a, 10) 

print ("a: ", a) # 객체정보 출력
print ("b: ", b) # 객체정보 출력

with tf.Session() as sess:
   sess.run(tf.global_variables_initializer())

   print (sess.run(a)) # a만 실행(a==1)
   print (sess.run(b)) # b실행(assign 연산 실행) (b==10, a==10)
   print (sess.run(a)) # b를 실행한 이후이므로 a는 변경된 10을 출력
```

```python
import tensorflow as tf

a = tf.Variable(1)
b = a * 3
c = tf.assign(a, tf.add(a, 1))

sess = tf.Session()
sess.run(tf.global_variables_initializer())

print (sess.run(a)) # a <- 1
print (sess.run(c)) # a <- 2, c <- 2 
print (sess.run(b)) # b <- 2 * 3
print (sess.run(a)) # a <- 2
```

- tensor 객체와 python 객체를 연산하면 python 객체가 tensor 객체로 변환된다.

### Tensor와 일반 python 변수의 차이 비교

```python
import tensorflow as tf
# 상수 정의하기 
a = tf.constant(120)
b = tf.constant(130)
c = tf.constant(140)
# 변수 정의하기 
v = tf.Variable(0)
v = tf.assign(v, 1)
j = v+1
# 데이터 플로우 그래프 정의하기 
k=a  #k는 a라는 상수를 가리키는 변수
v=1  #v는 더 이상 tensor를 가리키지 않음. 일반 변수임
print(v) #실행됨
k=1
print(k) #실행됨
print(a) # 타입만 출력되고 실제값은 안나옴. session을 실행해야 결과 출력됨

o = tf.add(1,1)  # 1은 파이썬 상수이고 o는 tensor임. 
                 # tf 연산자에 피연산자가 일반 상수가 올 수 있음. 단 결과는 텐서로 만들어짐
                 # o = 1+1은 o가 텐서가 아님. 파이썬 변수임. 단 두개의 1중에 하나라도 텐서이면 o는 텐서가 됨
# 세션 실행하기 
sess = tf.Session()
#sess.run(v)
# v의 내용 출력하기 
#print( sess.run(k) ) # error: k는 tensor가 아님
#print( sess.run(v) ) # error: v는 tensor가 아님
print( sess.run(j) ) # 정상
print(sess.run(o)) # 정상
```

## placeholder

```python
import tensorflow as tf

# float type의 placeholder 정의
a = tf.placeholder("float")
b = tf.placeholder("float")

y = tf.add(a, b)

with tf.Session() as sess:
       result = sess.run(y, feed_dict={a: 3, b: 3})  # placeholder에 값을 대입하여 연산
       print(result)
```

```python
import tensorflow as tf

a = tf.placeholder(tf.int32, [3])

b = tf.constant(2)
x2_op = a * b

with tf.Session() as sess:
    r1 = sess.run(x2_op, feed_dict={ a:[1, 2, 3] })
    print(r1)
    r2 = sess.run(x2_op, feed_dict={ a:[10, 20, 10] })
    print(r2)
```

```python
import tensorflow as tf

# 플레이스홀더 정의
# 배열의 크기를 None으로 지정. 1차원 배열이란 것만 명시
a = tf.placeholder(tf.int32, [None])
# 배열의 모든 값을 10배하는 연산 정의하기 

b = tf.constant(10)
x10_op = a * b

with tf.Session() as sess:
     # 플레이스홀더에 값을 넣어 실행
     r1 = sess.run(x10_op, feed_dict={a: [1,2,3,4,5]})
     print(r1)
     r2 = sess.run(x10_op, feed_dict={a: [10,20]})
     print(r2)
```

# Softmax

![4%E1%84%80%E1%85%A1%E1%86%BC%20tensorflow%20%E1%84%80%E1%85%B5%E1%84%8E%E1%85%A9%205764296313454fc49857119c864e33cb/Untitled.png](4%E1%84%80%E1%85%A1%E1%86%BC%20tensorflow%20%E1%84%80%E1%85%B5%E1%84%8E%E1%85%A9%205764296313454fc49857119c864e33cb/Untitled.png)

- 지수함수의 효과로 큰 값은 더 크게, 작은 값은 더 작게 만드는 효과를 가짐
- 그러면서 0~1사이의 값들로 스케일링 하고, 결과들의 합이 1 이 되도록 함

# Cost Function 설계

## Cross Entropy

![4%E1%84%80%E1%85%A1%E1%86%BC%20tensorflow%20%E1%84%80%E1%85%B5%E1%84%8E%E1%85%A9%205764296313454fc49857119c864e33cb/Untitled%201.png](4%E1%84%80%E1%85%A1%E1%86%BC%20tensorflow%20%E1%84%80%E1%85%B5%E1%84%8E%E1%85%A9%205764296313454fc49857119c864e33cb/Untitled%201.png)

- 지수함수의 결과를 로그로 상쇄시킴

## Cost Function

![4%E1%84%80%E1%85%A1%E1%86%BC%20tensorflow%20%E1%84%80%E1%85%B5%E1%84%8E%E1%85%A9%205764296313454fc49857119c864e33cb/Untitled%202.png](4%E1%84%80%E1%85%A1%E1%86%BC%20tensorflow%20%E1%84%80%E1%85%B5%E1%84%8E%E1%85%A9%205764296313454fc49857119c864e33cb/Untitled%202.png)

⇒ 결국 Logistic cost를 확장한 형태가 Cross entropy 임

![4%E1%84%80%E1%85%A1%E1%86%BC%20tensorflow%20%E1%84%80%E1%85%B5%E1%84%8E%E1%85%A9%205764296313454fc49857119c864e33cb/Untitled%203.png](4%E1%84%80%E1%85%A1%E1%86%BC%20tensorflow%20%E1%84%80%E1%85%B5%E1%84%8E%E1%85%A9%205764296313454fc49857119c864e33cb/Untitled%203.png)

# 정리

![4%E1%84%80%E1%85%A1%E1%86%BC%20tensorflow%20%E1%84%80%E1%85%B5%E1%84%8E%E1%85%A9%205764296313454fc49857119c864e33cb/Untitled%204.png](4%E1%84%80%E1%85%A1%E1%86%BC%20tensorflow%20%E1%84%80%E1%85%B5%E1%84%8E%E1%85%A9%205764296313454fc49857119c864e33cb/Untitled%204.png)

- 마지막 label 은 one-hot encoding이 된 상태의 결과여야 한다.

정규화를 하는 두가지 방법: minmax, 0~1 

정규화는 하는 것이 바람직하지만 어느 것이 더 좋은지는 알 수 없다. 

# Logistic regression mnist

```python
# 55000개의 mnist 데이터를 테스트. 각 cell은 0과 1 사이의 실수로 구성됨
# mnist 데이터는 tensorflow에서 자동으로 읽어오는 함수들이 존재함
# mnist를 softmax로 실험하기
# epoch와 batch 개념 알기(epoch는 학습 반복횟수를 의미함. batch는 한 학습을 여러개로 나누어(데이타가 많으므로)
# 구별해서 학습할 목적임

import os
os.environ['TF_CPP_MIN_LOG_LEVEL']='2'

import tensorflow as tf
import random
# import matplotlib.pyplot as plt
tf.set_random_seed(777)  # for reproducibility

from tensorflow.examples.tutorials.mnist import input_data

# Check out https://www.tensorflow.org/get_started/mnist/beginners for
# more information about the mnist dataset
# one_hot형태로 읽어올 것(학습과 테스트 데이터의 라벨 모두)
mnist = input_data.read_data_sets("MNIST_data/", one_hot=True)

# mnist.train과 mnist.test로 구성. 각각의 원소들을 읽어오는 method가 존재함(mnist는 class임)
# mnist는 테스트용으로 다운받아 곧바로 사용할 수 있도록 class로 구성되어 있으므로
# real 데이터에 대한 실험방법은 별도로 공부해야함

nb_classes = 10

# MNIST data image of shape 28 * 28 = 784
X = tf.placeholder(tf.float32, [None, 784])
# 0 - 9 digits recognition = 10 classes
Y = tf.placeholder(tf.float32, [None, nb_classes])

W = tf.Variable(tf.random_normal([784, nb_classes]))
b = tf.Variable(tf.random_normal([nb_classes]))

# Hypothesis (using softmax)
hypothesis = tf.nn.softmax(tf.matmul(X, W) + b)

cost = tf.reduce_mean(-tf.reduce_sum(Y * tf.log(hypothesis), axis=1))
optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)

# Test model
# Y는 one hot으로 읽어왔음.
is_correct = tf.equal(tf.arg_max(hypothesis, 1), tf.arg_max(Y, 1))
# Calculate accuracy
accuracy = tf.reduce_mean(tf.cast(is_correct, tf.float32))

# parameters
training_epochs = 15   #반복할 학습 획수
batch_size = 100  #한번에 학습할 사이즈(사이즈가 매우 클때
```

```python
with tf.Session() as sess:
    # Initialize TensorFlow variables
    sess.run(tf.global_variables_initializer())
    # Training cycle
    for epoch in range(training_epochs):
        avg_cost = 0
        total_batch = int(mnist.train.num_examples / batch_size) # batch횟수 계산

        for i in range(total_batch):
            batch_xs, batch_ys = mnist.train.next_batch(batch_size)
            c, _ = sess.run([cost, optimizer], feed_dict={
                            X: batch_xs, Y: batch_ys})
            avg_cost += c / total_batch  # cost는 평균이니까 다시 그것들의 평균을 구함

        print('Epoch:', '%04d' % (epoch + 1),
              'cost =', '{:.9f}'.format(avg_cost)) # 문자열 format은 따로 공부해야함

    print("Learning finished")

    # Test the model using test sets
    # 원래는 sess.run으로 실행하나 accuracy.eval와 같이 tensor의 eval명령으로 직접 돌릴 수 있음. 같은 의미임
    print("Accuracy: ", accuracy.eval(session=sess, feed_dict={
          X: mnist.test.images, Y: mnist.test.labels}))

    # Get one and predict
    # 하나의 테스트 데이터를 임의로 가져와서 일치하는지 테스트
    r = random.randint(0, mnist.test.num_examples - 1) #난수발생
    # 난수를 이용하여 테스트 집합에서 하나를 가져옴. one_hot으로 읽어왔으므로 다시 argmax로 index알아냄
    print("Label: ", sess.run(tf.argmax(mnist.test.labels[r:r + 1], 1)))
    print("Prediction: ", sess.run(
        tf.argmax(hypothesis, 1), feed_dict={X: mnist.test.images[r:r + 1]}))
```

- 데이터가 크기 때문에 batch 로 잘라서 학습한다.