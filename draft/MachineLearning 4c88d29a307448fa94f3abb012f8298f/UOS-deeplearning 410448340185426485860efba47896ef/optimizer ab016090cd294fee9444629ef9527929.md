# optimizer

# stochastic gradient descent

- 임의로 데이터 샘플을 가져와 학습에 사용
- learning rate를 동적으로 바꿀 수 있지 않을까?
- 물리학의 관성의 성질을 이용
- 이전의 gradient가 충분히 크면 local minimum을 지나칠 것이라는 아이디어

# Adagrad(adaptive gradient)

- 각각의 가중치 마다 learning rate를 다르게 주는 방식
- 이전까지 많이 변화한 값은 적게 변화하도록 하고 이전까지 적게 변화한 값은 많이 변화하도록 함
- 학습이 계속 될수록 G값은 계속 커지므로 업데이트가 느려지는 단점
- 초반에는 좋은데 나중에 안좋아짐

![optimizer%20ab016090cd294fee9444629ef9527929/Untitled.png](optimizer%20ab016090cd294fee9444629ef9527929/Untitled.png)

# RMSProp

- Adagrad 단점을 보완
- G를 지수 이동 평균으로 설정
- 감마는 보통 0.99나 0.9로 설정

![optimizer%20ab016090cd294fee9444629ef9527929/Untitled%201.png](optimizer%20ab016090cd294fee9444629ef9527929/Untitled%201.png)

# Adam

- Momentum과 RMSProp을 합친 알고리즘
- 이게 최근에 가장 좋은것으로 보입니다.