# WEEK4

# Deep L-layer neural network

![WEEK4%200cc4b062510f4acd94dcf23d7aa95f59/Untitled.png](WEEK4%200cc4b062510f4acd94dcf23d7aa95f59/Untitled.png)

- 로지스틱 리그레션은 기술적으로 레이어 하나짜리 뉴럴 네트워크이다.

# Forward Propagation in a Deep Network

![WEEK4%200cc4b062510f4acd94dcf23d7aa95f59/Untitled%201.png](WEEK4%200cc4b062510f4acd94dcf23d7aa95f59/Untitled%201.png)

- Forward propagation 을 하려면 for-loop 가 필요해 보인다.
- explicit for loop는 피하는 것이 좋지만 이 상황에서는 피하는 방법이 없는 것처럼 보인다.

![WEEK4%200cc4b062510f4acd94dcf23d7aa95f59/Untitled%202.png](WEEK4%200cc4b062510f4acd94dcf23d7aa95f59/Untitled%202.png)

- 레이어에서 레이어로 넘어가는 wegith matrix 의 dimension 계산
- 구현할때 중요한 디버깅 포인트가 된다.

    $$a^{[l]} = g^{[l]}(z^{[l]})$$

    - and a and z has dimension (n^l, 1)

![WEEK4%200cc4b062510f4acd94dcf23d7aa95f59/1569394254931.png](WEEK4%200cc4b062510f4acd94dcf23d7aa95f59/1569394254931.png)

- 왜 딥러닝이 잘 동작할까?
    - XOR 같은 연산의 경우 많은 레이어를 통해 계산을 하는 것보다 하나의 레이어에서 계산을 하는 것이 더 효율적인다.
    - 브랜딩 인거 같기도 함

![WEEK4%200cc4b062510f4acd94dcf23d7aa95f59/Untitled%203.png](WEEK4%200cc4b062510f4acd94dcf23d7aa95f59/Untitled%203.png)

![WEEK4%200cc4b062510f4acd94dcf23d7aa95f59/Untitled%204.png](WEEK4%200cc4b062510f4acd94dcf23d7aa95f59/Untitled%204.png)

$$\operatorname{d}\!a^{[l]} = \frac{y}{a} - \frac{1-y}{1-a}$$

![WEEK4%200cc4b062510f4acd94dcf23d7aa95f59/Untitled%205.png](WEEK4%200cc4b062510f4acd94dcf23d7aa95f59/Untitled%205.png)

- 최적의 하이퍼 파라미터를 찾는 것은 여전히 어려운 일이다.
- 실험적으로 얻어야 하며 이런 분야로 여전히 딥러닝은 발전중에 있다.

$$dZ  [L]  =A  [L]  −Y  dW^{[L]} = \frac{1}{m}dZ^{[L]}{A^{[L-1]^{T}}}dW  [L]  =  m 1 ​	 dZ  [L]  A  [L−1]  T      db^{[L]} = \frac{1}{m} np.sum(dZ^{[L]}, axis=1, keepdims=True)db  [L]  =  m 1 ​	 np.sum(dZ  [L]  ,axis=1,keepdims=True)  dZ^{[L-1]} = W^{[L]^{T}}dZ^{[L]} * g'^{[L-1]}(Z^{[L-1]})dZ  [L−1]  =W  [L]  T    dZ  [L]  ∗g  ′[L−1]  (Z  [L−1]  )$$