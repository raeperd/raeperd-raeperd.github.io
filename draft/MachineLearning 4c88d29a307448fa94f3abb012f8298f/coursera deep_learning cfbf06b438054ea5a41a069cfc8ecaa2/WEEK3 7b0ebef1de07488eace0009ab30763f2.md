# WEEK3

# SUMMARY

- tanh 가 sigmoid 보다 좋은 성능을 낸다.
- 활성화 함수로 쓰이는 친구들은 미분값을 구하기가 쉬운친구들이다.
- 초기화를 잘해야 한다.

![WEEK3%207b0ebef1de07488eace0009ab30763f2/Untitled.png](WEEK3%207b0ebef1de07488eace0009ab30763f2/Untitled.png)

- tanh 가 sigmoid 보다 대부분의 경우 좋은 값을 낸다.
- 평균을 0으로 맞춰주기 때문에 데이터를 중앙화 할 수 있음
- binary classfiation 에서 output layer에 사용하는 것이 아니라면 sigmoid는 사용하지 않는다.

![WEEK3%207b0ebef1de07488eace0009ab30763f2/Untitled%201.png](WEEK3%207b0ebef1de07488eace0009ab30763f2/Untitled%201.png)

tanh 나 sigmoid 모두 z가 아주 크거나 아주 작으면 미분값이 0에 가까워진다.

relu의 0에서의 미분값은 수학적으로는 존재하지 않지만 수치적으로 존재한다고 생각할 수 있다. 

relu 가 거의 디폴트로 사용된다. 

Leaky Relu 도 잘 먹힘 

⇒ 기울기가 0과 다르기때문에 좋다

application마다 다를 수 있으니 모두 시도해보라 

![WEEK3%207b0ebef1de07488eace0009ab30763f2/Untitled%202.png](WEEK3%207b0ebef1de07488eace0009ab30763f2/Untitled%202.png)

- 선형 함수의 합성은 여전히 선형이기 떄문에 선형 활성함수는 큰 의미가 없다.

activatoin function의 slope를 구하는 방법

![WEEK3%207b0ebef1de07488eace0009ab30763f2/Untitled%203.png](WEEK3%207b0ebef1de07488eace0009ab30763f2/Untitled%203.png)

- sigmoid의 미분값은 원래 함수값을 이용해서 구할 수 있다.

![WEEK3%207b0ebef1de07488eace0009ab30763f2/Untitled%204.png](WEEK3%207b0ebef1de07488eace0009ab30763f2/Untitled%204.png)

- 이 친구도 마찬가지다.

 

![WEEK3%207b0ebef1de07488eace0009ab30763f2/Untitled%205.png](WEEK3%207b0ebef1de07488eace0009ab30763f2/Untitled%205.png)

- 입력값을 이용해 미분값을 구하는 것은 아니지만 상수시간안에 구할 수 있다.

1. 주어진 수를 소인수 분해한다. L * sqrt(n)
2. 정렬한다 L * log(L)
3. map 으로 알파벳과 맵핑 
4. decript

![WEEK3%207b0ebef1de07488eace0009ab30763f2/Untitled%206.png](WEEK3%207b0ebef1de07488eace0009ab30763f2/Untitled%206.png)

- keepdims = True 를 통해 차원이 정의되지 않은 배열이 나오는 것을 막을 수 있다.

![WEEK3%207b0ebef1de07488eace0009ab30763f2/Untitled%207.png](WEEK3%207b0ebef1de07488eace0009ab30763f2/Untitled%207.png)

- read right to left
- by chain rule, we get dz = a-y

![WEEK3%207b0ebef1de07488eace0009ab30763f2/Untitled%208.png](WEEK3%207b0ebef1de07488eace0009ab30763f2/Untitled%208.png)

![WEEK3%207b0ebef1de07488eace0009ab30763f2/Untitled%209.png](WEEK3%207b0ebef1de07488eace0009ab30763f2/Untitled%209.png)

![WEEK3%207b0ebef1de07488eace0009ab30763f2/Untitled%2010.png](WEEK3%207b0ebef1de07488eace0009ab30763f2/Untitled%2010.png)

- weight와 b를 모두 0으로 초기화하면 같은 함수를 계산하게 된다.
- neural network being meaningless

![WEEK3%207b0ebef1de07488eace0009ab30763f2/Untitled%2011.png](WEEK3%207b0ebef1de07488eace0009ab30763f2/Untitled%2011.png)

- 모두 0이 아니더라도 아주 작거나 아주 큰값을 초기값으로 설정해서는 안된다.
- 보통은 weight의 초깃값에 0.01 정도를 곱한다.
- 간단한 적은 layerd system 에서는 잘 먹히지만 복잡한, deep system에서는 0.01이 아닌 다른 상수를 선택해야할 수도 있다. (⇒ 다음주에 배웁니다.)

# Quize review

- Logistic regression 은 0으로 초기화 해도 괜찮다.
- activation node의 수가 2→4 로 간다고 해보자
    - w.shape == (4,2)
    - b.shape == (4,1)
    - A.shape == Z.shape == (4,m)