# Week3

# Week3 Summary

- Classification needs different form of hypothesis representation
- Deicision Boundary is property of the parameters not the dataset
- Cost function of Logistic regression model and advanced optimization algorithm
- Adress overfitting ⇒ less feature, reguralization

---

# Classification and Representation

## Hypothesis Representation

### motivation

[Week3%207eea2e643a604a75a66ab57b67dc65e0/untitled](Week3%207eea2e643a604a75a66ab57b67dc65e0/untitled)

- Linear regression is not enough

### Logistic Function a.k.a. Sigmoid Function

Want 0≤h(theta, x) ≤ 1 

[Week3%207eea2e643a604a75a66ab57b67dc65e0/untitled%201](Week3%207eea2e643a604a75a66ab57b67dc65e0/untitled%201)

- Maps any real number to the (0,1) interval

[Week3%207eea2e643a604a75a66ab57b67dc65e0/untitled%202](Week3%207eea2e643a604a75a66ab57b67dc65e0/untitled%202)

- hypothesis will give us probabillity that our output is 1.

## Decision Boundary

[Week3%207eea2e643a604a75a66ab57b67dc65e0/untitled%203](Week3%207eea2e643a604a75a66ab57b67dc65e0/untitled%203)

The way our logistic function g behaves is that when its input is greater than or equal to zero, its output is greater than or equal to 0.5:

[Week3%207eea2e643a604a75a66ab57b67dc65e0/untitled%204](Week3%207eea2e643a604a75a66ab57b67dc65e0/untitled%204)

[Week3%207eea2e643a604a75a66ab57b67dc65e0/untitled%205](Week3%207eea2e643a604a75a66ab57b67dc65e0/untitled%205)

### Example of Decision Boundary

[Week3%207eea2e643a604a75a66ab57b67dc65e0/untitled%206](Week3%207eea2e643a604a75a66ab57b67dc65e0/untitled%206)

- In this example decision boundary is x1=5
- Decision Boundary is a not a propoerty of dataset but of parameters

---

# Logistic Regression Model

## Cost Function

We cannot use the same cost function(1/2*squre error) that we use for linear regression because it will not be a convex function. Instead, our cost function for logistic regression looks like:

[Week3%207eea2e643a604a75a66ab57b67dc65e0/untitled%207](Week3%207eea2e643a604a75a66ab57b67dc65e0/untitled%207)

### when y = 1 and y = 0

[Week3%207eea2e643a604a75a66ab57b67dc65e0/untitled%208](Week3%207eea2e643a604a75a66ab57b67dc65e0/untitled%208)

[Week3%207eea2e643a604a75a66ab57b67dc65e0/untitled%209](Week3%207eea2e643a604a75a66ab57b67dc65e0/untitled%209)

### Simplified Cost Function

[Week3%207eea2e643a604a75a66ab57b67dc65e0/untitled%2010](Week3%207eea2e643a604a75a66ab57b67dc65e0/untitled%2010)

- And then, gradient descent works same with new cost function
- Cost function of logistic regression is derived from the priciple of maximum likelihood estimation

---

## Advanced Optimization

"Conjugate gradient", "BFGS", and "L-BFGS" are more sophisticated, faster ways to optimize θ that can be used instead of gradient descent. 

To use advanced optimization, We first need to provide a function that evaluates the following two functions for a given input value θ:

[Week3%207eea2e643a604a75a66ab57b67dc65e0/untitled%2011](Week3%207eea2e643a604a75a66ab57b67dc65e0/untitled%2011)

First define cost function

```matlab
function [jVal, gradient] = costFunction(theta)
  jVal = [...code to compute J(theta)...];
  gradient = [...code to compute derivative of J(theta)...];
end
```

call fminunc() 

```matlab
options = optimset('GradObj', 'on', 'MaxIter', 100);
initialTheta = zeros(2,1);
   [optTheta, functionVal, exitFlag] = fminunc(@costFunction, initialTheta, options);
```

---

# Multiclass Classification: One vs all

Since y = {0,1...n}, we divide our problem into n+1 (+1 because the index starts at 0) binary classification problems; in each one, we predict the probability that 'y' is a member of one of our classes.

[Week3%207eea2e643a604a75a66ab57b67dc65e0/untitled%2012](Week3%207eea2e643a604a75a66ab57b67dc65e0/untitled%2012)

[Week3%207eea2e643a604a75a66ab57b67dc65e0/untitled%2013](Week3%207eea2e643a604a75a66ab57b67dc65e0/untitled%2013)

1. Train a logistic regression classifier for each class to predict the probabillity that y = i 
2. To make a prediction on a new x, pick the class that maximizes h(theta,x)^(k)

---

# Solving the Problem of Overfitting

## The Problem of Overfitting

[Week3%207eea2e643a604a75a66ab57b67dc65e0/untitled%2014](Week3%207eea2e643a604a75a66ab57b67dc65e0/untitled%2014)

Without formally defining what these terms mean, we’ll say the figure on the left shows an instance of **underfitting(high bias)**—in which the data clearly shows structure not captured by the model—and the figure on the right is an example of **overfitting(high variance).**

Overfitting is caused by a complicated function that creates a lot of unnecessary curves and angles unrelated to the data.

### Two main options to address the isuue of overfitting

1) Reduce the number of features:

- Manually select which features to keep.
- Use a model selection algorithm (studied later in the course).

2) Regularization

- Keep all the features, but reduce the magnitude of parameters theta, start subscript, j, end subscript*θj*​.

    \theta_j

- Regularization works well when we have a lot of slightly useful features.

## Regularization

[Week3%207eea2e643a604a75a66ab57b67dc65e0/untitled%2015](Week3%207eea2e643a604a75a66ab57b67dc65e0/untitled%2015)

- where lamda is **regularization parameter**. It determines how much the costs of our theta parameters are inflated.

## Regularized Linear Regresson

### Gradient Descent

[Week3%207eea2e643a604a75a66ab57b67dc65e0/untitled%2016](Week3%207eea2e643a604a75a66ab57b67dc65e0/untitled%2016)

- We do not one to penalize theta 0.

### Normal Equation

[Week3%207eea2e643a604a75a66ab57b67dc65e0/untitled%2017](Week3%207eea2e643a604a75a66ab57b67dc65e0/untitled%2017)

- if m<n then X^t*X is non-invertible. However with lamda*L then it becomes invertible.

## Regularized Logistic Regression

[Week3%207eea2e643a604a75a66ab57b67dc65e0/untitled%2018](Week3%207eea2e643a604a75a66ab57b67dc65e0/untitled%2018)

- The second sum means to explicitly exclude the bias term.

[Week3%207eea2e643a604a75a66ab57b67dc65e0/untitled%2019](Week3%207eea2e643a604a75a66ab57b67dc65e0/untitled%2019)

[Lecture7.pdf](Week3%207eea2e643a604a75a66ab57b67dc65e0/Lecture7.pdf)